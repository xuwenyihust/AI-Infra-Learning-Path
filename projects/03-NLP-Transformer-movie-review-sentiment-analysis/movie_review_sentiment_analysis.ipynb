{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Review Sentiment Analysis"
      ],
      "metadata": {
        "id": "edOg-pYRLhGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Management"
      ],
      "metadata": {
        "id": "TMkSvRHSPE70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check installed versions\n",
        "!pip list | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXPM7vjrNPFH",
        "outputId": "42dba20e-b4e8-4bff-ccb8-1f15129af42a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                                 2.3.1+cu121\n",
            "torchao                               0.10.0\n",
            "torchdata                             0.11.0\n",
            "torchsummary                          1.5.1\n",
            "torchtext                             0.18.0\n",
            "torchtune                             0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell fixes the torch/torchtext compatibility issue in Colab.\n",
        "# Run this once at the beginning of your session.\n",
        "!pip uninstall -y torch torchtext torchaudio torchvision\n",
        "!pip install torch==2.3.1 torchtext==0.18.0 --extra-index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CuPN9KZ_ONiu",
        "outputId": "326e9624-1c08-4e70-b3ed-710ad8095662"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.3.1+cu121\n",
            "Uninstalling torch-2.3.1+cu121:\n",
            "  Successfully uninstalled torch-2.3.1+cu121\n",
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.1\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
            "Collecting torchtext==0.18.0\n",
            "  Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: torch, torchtext\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.19 requires torchvision, which is not installed.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.3.1+cu121 torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In-kOdg_eZcZ",
        "outputId": "f6564f0e-687a-47d3-c4ae-4a23f07a548c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "LOPncUfOPMyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "EMBEDDING_DIM = 256 # Also known as d_model in Transformers\n",
        "HIDDEN_DIM = 512 # Size of the feed-forward network\n",
        "N_CLASSES = 2\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4 # Transformers often benefit from a smaller learning rate\n",
        "MODEL_SAVE_PATH = \"sentiment_transformer.pth\""
      ],
      "metadata": {
        "id": "DUcxMYPYLeuY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer-specific config\n",
        "N_HEADS = 4 # Number of attention heads\n",
        "N_ENCODER_LAYERS = 6 # Number of Transformer Encoder layers\n",
        "DROPOUT = 0.1"
      ],
      "metadata": {
        "id": "h6hac6KwLewx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmMSzWNxLezm",
        "outputId": "3a2bbff1-a098-4db7-fbd5-a3bccc4e8638"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "8W9QYlRwPi_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset and building vocabulary...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "train_iter = (item['text'] for item in dataset['train'])\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCAB_SIZE)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4Oe1wIYPS-4",
        "outputId": "a180d59e-471d-4d63-f18a-46cfc9f7445f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset and building vocabulary...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImdbFromScratchDataset(Dataset):\n",
        "    def __init__(self, dataset_split, vocab, tokenizer):\n",
        "        self.dataset = dataset_split\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        text = item['text']\n",
        "        label = item['label']\n",
        "        token_ids = self.vocab(self.tokenizer(text))\n",
        "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "IUB5coZzU9fn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    labels, texts = [], []\n",
        "    for (text, label) in batch:\n",
        "        labels.append(label)\n",
        "        texts.append(text)\n",
        "\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    texts_padded = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    return texts_padded, labels_tensor"
      ],
      "metadata": {
        "id": "XCU3qGFHU_FZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImdbFromScratchDataset(dataset['train'], vocab, tokenizer)\n",
        "test_dataset = ImdbFromScratchDataset(dataset['test'], vocab, tokenizer)"
      ],
      "metadata": {
        "id": "zpSsJzuIVBo-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(\"Data pipeline prepared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu4czBeOVBrs",
        "outputId": "35c19932-f277-442d-cd8a-60a222967013"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Data pipeline prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "HygmZsM3PmdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [seq_len, batch_size, embedding_dim]\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "4b_F-HSlPTD9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_encoder_layers, hidden_dim, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
        "\n",
        "        # Define a single Transformer Encoder Layer\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        # Stack multiple encoder layers together\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=n_encoder_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, output_dim)\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch size, seq len]\n",
        "\n",
        "        # Generate a mask for the padding tokens\n",
        "        # The mask should be True for tokens that should be ignored\n",
        "        src_key_padding_mask = (text == vocab['<pad>'])\n",
        "\n",
        "        embedded = self.embedding(text) * math.sqrt(self.d_model)\n",
        "        # embedded shape: [batch size, seq len, embedding dim]\n",
        "\n",
        "        # Note: PyTorch TransformerEncoder expects [seq_len, batch_size, embed_dim]\n",
        "        # if batch_first=False (default). We are using batch_first=True.\n",
        "        positioned = self.pos_encoder(embedded)\n",
        "\n",
        "        # Pass through the Transformer Encoder\n",
        "        encoder_output = self.transformer_encoder(positioned, src_key_padding_mask=src_key_padding_mask)\n",
        "        # encoder_output shape: [batch size, seq len, embedding dim]\n",
        "\n",
        "        # We will use the output of the first token ([CLS] token's equivalent) for classification\n",
        "        # or we can average all the outputs\n",
        "        pooled_output = encoder_output.mean(dim=1)\n",
        "        # pooled_output shape: [batch size, embedding dim]\n",
        "\n",
        "        return self.fc(pooled_output)"
      ],
      "metadata": {
        "id": "eLf4PT36PTGw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerSentimentClassifier(\n",
        "    len(vocab), EMBEDDING_DIM, N_HEADS, N_ENCODER_LAYERS, HIDDEN_DIM, N_CLASSES, DROPOUT, vocab['<pad>']\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "2XyovzTSPuKt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Methods"
      ],
      "metadata": {
        "id": "_EP3KtqRPu_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)"
      ],
      "metadata": {
        "id": "WWsztm4SPuNb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, labels in tqdm(iterator, desc=\"Training\"):\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "jShIfb4JPuPx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for text, labels in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(predictions, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss / len(iterator), accuracy"
      ],
      "metadata": {
        "id": "Zq3T6H0APuSl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Training Loop"
      ],
      "metadata": {
        "id": "-Aex2tbrPyFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    print(f'--- Epoch {epoch + 1}/{EPOCHS} ---')\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_accuracy = eval_model(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print(f'Training Loss: {train_loss:.4f}')\n",
        "    print(f'Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}')\n",
        "    print(f\"Epoch {epoch + 1} duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Total training time: {(total_end_time - total_start_time):.2f} seconds\")\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU92LufKPuU7",
        "outputId": "0300d6b6-74b8-4de8-a885-15f48dd3cf18"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "--- Epoch 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [07:32<00:00,  1.73it/s]\n",
            "Evaluating: 100%|██████████| 782/782 [00:29<00:00, 26.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5366\n",
            "Validation Loss: 0.5697 | Validation Accuracy: 0.7592\n",
            "Epoch 1 duration: 481.70 seconds\n",
            "--- Epoch 2/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [07:32<00:00,  1.73it/s]\n",
            "Evaluating: 100%|██████████| 782/782 [00:29<00:00, 26.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4117\n",
            "Validation Loss: 0.5511 | Validation Accuracy: 0.7544\n",
            "Epoch 2 duration: 481.70 seconds\n",
            "--- Epoch 3/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [07:32<00:00,  1.73it/s]\n",
            "Evaluating: 100%|██████████| 782/782 [00:29<00:00, 26.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3684\n",
            "Validation Loss: 0.4953 | Validation Accuracy: 0.8165\n",
            "Epoch 3 duration: 481.19 seconds\n",
            "\n",
            "--- Training Complete ---\n",
            "Total training time: 1444.59 seconds\n",
            "Model saved to sentiment_transformer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfP1qW6YExbi"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}